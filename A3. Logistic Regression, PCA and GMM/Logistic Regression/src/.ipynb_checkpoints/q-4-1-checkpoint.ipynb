{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomDataSet = dataSet = pd.read_csv(\"./../input_data/AdmissionDataset/data.csv\")\n",
    "# randomDataSet = dataSet.sample(frac=1).reset_index(drop=True)\n",
    "Class = \"Chance of Admit\"\n",
    "columns = ['Serial No.' , 'GRE Score' , 'TOEFL Score' , 'University Rating' , 'SOP' , 'LOR' , 'CGPA' , 'Research' , Class]\n",
    "randomDataSet.columns = columns\n",
    "columns = columns[1:]\n",
    "randomDataSet = randomDataSet[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in columns[:-1]:\n",
    "    randomDataSet[attr] = (randomDataSet[attr] - randomDataSet[attr].min())/(randomDataSet[attr].max() - randomDataSet[attr].min())\n",
    "# randomDataSet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet, validationSet = np.split(randomDataSet, [int(0.8*len(randomDataSet))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validationSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterate = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_hx(theta, row):\n",
    "    hx = 0\n",
    "    n = len(theta)\n",
    "    for i in range(0, n - 1):\n",
    "        hx += theta[i]*row[i]\n",
    "    return hx + theta[n - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(df, theta):\n",
    "    error_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        y = row[Class]\n",
    "        hx = hypothesis_hx(theta, row)\n",
    "        error_list.append(hx - y)\n",
    "    return error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(df, theta):\n",
    "    Jtheta = 0\n",
    "    er = 0\n",
    "    error_list = error(df, theta)\n",
    "    m = len(error_list)\n",
    "    for i in range(0, m):\n",
    "        er += error_list[i]**2\n",
    "    Jtheta = er/(2*m + eps)\n",
    "    return Jtheta , error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumError(df, error_list, j):\n",
    "    m = len(error_list)\n",
    "    n = len(theta)\n",
    "    sum_error = 0\n",
    "    for i in range(0, m):\n",
    "        if j == n - 1:\n",
    "            sum_error += error_list[i]\n",
    "        else:\n",
    "            sum_error += error_list[i]*df.iloc[i][j]\n",
    "    return sum_error/(m + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDecent(df, theta, error_list):\n",
    "    n = len(theta)\n",
    "    for j in range(0, n):\n",
    "        theta[j] = theta[j] - alpha*sumError(df, error_list, j)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(label_x, label_y, title, x_axis, y_axis, mark='', colr = 'blue'):\n",
    "    plt.figure(num=None, figsize=(6, 4), dpi=175, facecolor='w', edgecolor='k')\n",
    "    # plotting the points  \n",
    "    plt.plot(x_axis, y_axis, marker = mark, color = colr, label = 'Error rate') \n",
    "    # naming the x axis \n",
    "    plt.xlabel(label_x) \n",
    "    # naming the y axis \n",
    "    plt.ylabel(label_y) \n",
    "\n",
    "    # giving a title to my graph \n",
    "    plt.title(title) \n",
    "    plt.grid(True)\n",
    "    # function to show the plot \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting the Model\n",
    "theta = np.zeros([trainingSet.shape[1]])\n",
    "print(theta)\n",
    "iteration_list = []\n",
    "cost_list = []\n",
    "before = 10\n",
    "for i in range(iterate):\n",
    "    cost , error_list = costFunction(trainingSet, theta)\n",
    "    theta = gradientDecent(trainingSet, theta, error_list)\n",
    "    iteration_list.append(i)\n",
    "    cost_list.append(cost)\n",
    "    print(i,cost)\n",
    "#     print(theta)\n",
    "#     if before - cost < 0.0001:\n",
    "#         break\n",
    "#     before = cost\n",
    "plotter('Iterations','Cost','Iterations-vs-Cost',iteration_list,cost_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, featureRow):\n",
    "    n = len(theta)\n",
    "    y = 0\n",
    "    for i in range(0, n - 1):\n",
    "        y += theta[i]*featureRow[i]\n",
    "    return y + theta[n - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valdidation(validationSet,theta):\n",
    "    true_positive = 0\n",
    "    true_negative = 0 \n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for index, row in validationSet.iterrows():\n",
    "        if hypothesis_hx(theta, row) > 0.5:\n",
    "            pred = 1 \n",
    "        else:\n",
    "            pred = 0\n",
    "        if row[Class] > 0.5:\n",
    "            ac = 1\n",
    "        else:\n",
    "            ac = 0\n",
    "        if pred == ac:\n",
    "            if pred == 1:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                true_negative += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                false_positive += 1\n",
    "            else:\n",
    "                false_negative += 1\n",
    "        predicted.append(pred)\n",
    "        actual.append(ac)\n",
    "        print(pred)\n",
    "        print(ac)\n",
    "        print(\"-----------\")\n",
    "    return true_positive, true_negative , false_negative, false_positive, predicted, actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive, true_negative , false_negative, false_positive, predicted,actual = valdidation(validationSet,theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean square error\n",
    "def MSE(predicted , actual):\n",
    "    return np.mean((np.array(actual) - np.array(predicted))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean absolute error\n",
    "def MAE(predicted , actual):\n",
    "    return np.mean(np.abs((np.array(actual) - np.array(predicted))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute percentage error function\n",
    "def MAPE(predicted , actual):\n",
    "    return np.mean(np.abs((np.array(actual) - np.array(predicted))/(np.array(actual) + eps))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_positive , true_negative , false_negative, false_positive):\n",
    "    return ((true_positive + true_negative)*100)/(true_positive + true_negative + false_positive + false_negative + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(true_positive , false_negative):\n",
    "    return true_positive*100/(true_positive +  false_negative+ eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_positive , false_positive):\n",
    "    return true_positive*100/(true_positive +  false_positive + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1score(recall , prescision):\n",
    "    return 2/(1/(float(recall)+eps)+1/(float(prescision)+eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(true_positive , true_negative , false_negative, false_positive)\n",
    "rec = recall(true_positive , false_negative)\n",
    "pre = precision(true_positive , false_positive)\n",
    "f1 = f1score(rec, pre)\n",
    "\n",
    "print(\"------------Entropy------------\")\n",
    "print(\"ACCURACY%: \", acc)\n",
    "print(\"RECALL: \", rec)\n",
    "print(\"PRECISION: \", pre)\n",
    "print(\"F1-score: \", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataSet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dataSet.corr(),linewidth = 0.2, vmax=1.0, square=True, linecolor='red',annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
