{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(label_x, label_y, title, x_axis, y_axis, mark='', colr = 'blue'):\n",
    "    plt.figure(num=None, figsize=(6, 4), dpi=175, facecolor='w', edgecolor='k')\n",
    "    # plotting the points  \n",
    "    plt.plot(x_axis, y_axis, marker = mark, color = colr, label = 'Error rate') \n",
    "    # naming the x axis \n",
    "    plt.xlabel(label_x) \n",
    "    # naming the y axis \n",
    "    plt.ylabel(label_y) \n",
    "    # giving a title to my graph \n",
    "    plt.title(title) \n",
    "    plt.grid(True)\n",
    "    # function to show the plot \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(label_x, label_y, title, x_axis, y_axis, mark='.', colr = 'red'):\n",
    "    plt.figure(num=None, figsize=(6, 4), dpi=175, facecolor='w', edgecolor='k')\n",
    "    # plotting the y = 0 line  \n",
    "    plt.axhline(0, color='black')\n",
    "    # plotting points as a scatter plot \n",
    "    plt.scatter(x_axis, y_axis, label= \"stars\", color = colr, marker= mark, s=30) \n",
    "    # naming the x axis \n",
    "    plt.xlabel(label_x) \n",
    "    # naming the y axis \n",
    "    plt.ylabel(label_y) \n",
    "    # giving a title to my graph \n",
    "    plt.title(title) \n",
    "    plt.grid(True)\n",
    "    # function to show the plot \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalising the data set to reduce range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(randomDataSet, columns):\n",
    "    for attr in columns[:-1]:\n",
    "        randomDataSet[attr] = (randomDataSet[attr] - randomDataSet[attr].min())/(randomDataSet[attr].max() - randomDataSet[attr].min())\n",
    "    # randomDataSet = (randomDataSet - randomDataSet.mean())/randomDataSet.std()\n",
    "    # randomDataSet.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis function that defines equation of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_hx(theta, row):\n",
    "    hx = 0\n",
    "    n = len(theta)\n",
    "    for i in range(0, n - 1):\n",
    "        hx += theta[i]*row[i]\n",
    "    return hx + theta[n - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error between calculated value of hypothesis and actul value of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(df, theta):\n",
    "    error_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        y = row[Class]\n",
    "        hx = hypothesis_hx(theta, row)\n",
    "        error_list.append(hx - y)\n",
    "    return error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(df, theta):\n",
    "    Jtheta = 0\n",
    "    er = 0\n",
    "    error_list = error(df, theta)\n",
    "    m = len(error_list)\n",
    "    for i in range(0, m):\n",
    "        er += np.square(error_list[i])\n",
    "    Jtheta = er/(2*m)\n",
    "    return Jtheta , error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative function to calculate derivative of cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Derivative(df, error_list, theta,j):\n",
    "    m = len(error_list)\n",
    "    n = len(theta)\n",
    "    sum_error = 0\n",
    "    for i in range(0, m):\n",
    "        if j == n - 1:\n",
    "            sum_error += error_list[i]\n",
    "        else:\n",
    "            sum_error += error_list[i]*df.iloc[i][j]\n",
    "    return sum_error/(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Decent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDecent(df, theta, error_list):\n",
    "    n = len(theta)\n",
    "    for j in range(0, n):\n",
    "        theta[j] = theta[j] - alpha*Derivative(df, error_list, theta, j)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Model\n",
    "def modelFitting(trainingSet,iterate):\n",
    "    global theta\n",
    "    iteration_list = []\n",
    "    cost_list = []\n",
    "    before = 10\n",
    "    for i in range(iterate):\n",
    "        cost , error_list = costFunction(trainingSet, theta)\n",
    "        theta = gradientDecent(trainingSet, theta, error_list)\n",
    "        iteration_list.append(i)\n",
    "        cost_list.append(cost)\n",
    "        print(i,cost)\n",
    "    return iteration_list,cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valdidation Function for validating the data set for error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valdidation(validationSet,theta):\n",
    "    predicted = []\n",
    "    actual = []\n",
    "    for index, row in validationSet.iterrows():\n",
    "        pred = hypothesis_hx(theta, row)\n",
    "        ac = row[Class]\n",
    "        predicted.append(pred)\n",
    "        actual.append(ac)\n",
    "        print(pred)\n",
    "        print(ac)\n",
    "        print(\"-----------\")\n",
    "    return predicted , actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean square error\n",
    "def MSE(predicted , actual):\n",
    "    return np.mean((np.array(actual) - np.array(predicted))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error\n",
    "def MAE(predicted , actual):\n",
    "    return np.mean(np.abs((np.array(actual) - np.array(predicted))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute percentage error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute percentage error function\n",
    "def MAPE(predicted , actual):\n",
    "    return np.mean(np.abs((np.array(actual) - np.array(predicted))/ np.array(actual))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Plotter for verifying correctness of the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualPlotter(validationSet, predicted , actual):\n",
    "    residual = np.array(actual) - np.array(predicted)\n",
    "    for attr in validationSet:\n",
    "        scatterplot(attr,'Residual','Residual-vs-' + attr, validationSet[attr],residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomDataSet = dataSet = pd.read_csv(\"./../input_data/AdmissionDataset/data.csv\")\n",
    "randomDataSet = dataSet.sample(frac = 1).reset_index(drop = True)\n",
    "Class = \"Chance of Admit \"\n",
    "columns = ['Serial No.' , 'GRE Score' , 'TOEFL Score' , 'University Rating' , 'SOP' , 'LOR' , 'CGPA' , 'Research' , Class]\n",
    "randomDataSet.columns = columns\n",
    "columns = columns[1:]\n",
    "randomDataSet = randomDataSet[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for Data Analysis and model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat map for corelating the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dataSet.corr(),linewidth = 0.2, vmax=1.0, square=True, linecolor='red',annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(randomDataSet, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet, validationSet = np.split(randomDataSet, [int(0.8*len(randomDataSet))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting parameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iterate = 1000\n",
    "theta = np.zeros([trainingSet.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_list,cost_list = modelFitting(trainingSet,iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Cost vs Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter('Iterations','Cost','Cost-vs-Iterations',iteration_list,cost_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted , actual = valdidation(validationSet,theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE(predicted , actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Residual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ResidualPlotter(validationSet, predicted , actual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING function resturns list of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TESTING_algo(testingSet):\n",
    "    global theta\n",
    "    predicted = []\n",
    "    for index, row in testingSet.iterrows():\n",
    "        predicted.append(hypothesis_hx(theta, row))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST YOUR DATASET HERE....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(arr):\n",
    "    length = len(arr)\n",
    "    global columns\n",
    "    if length == 2:\n",
    "        testSetPath = arr[1]\n",
    "        testingData = pd.read_csv(testSetPath)\n",
    "        res = TESTING_algo(testingData)\n",
    "        testingData['RESULT'] = res\n",
    "        testingData.to_csv(\"./../output_data/q-3_data_output.csv\",index = False)\n",
    "    return testingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "#### 1) The error value ( which is calculated as the cost function in the code), becomes constant after 800 iterations.\n",
    "#### 2) Since the range of output label is very small, it makes the mean square error even smaller which is why the mean absolute error is greater than MSE.\n",
    "#### 3) The CGPA column  gives the best resdiual error distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
