{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Analyse how the two different regularisation techniques affect regression weights in terms of their values and what are the differences between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression do similar things, but in case of lasso regression, they make our predictions of output less sensitive to the tiny training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When labda is 0, then ridge and lasso regression, will be same as the Least Square Line. As lamvda increases in value, the slope gets smaller until the slope equals to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between Ridge and Lasso Regression is that Ridge Regression can only shrink the slope asymptotically close to 0 while Lasso Regression can shrink the slope all the way to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case of Ridge Regression we minimize :\n",
    "    The sum of squared residuals + (Lambda * (sum of slope^2)) \n",
    "\n",
    "Now if there are some important parameters, while some unimportant parameters, so on increasing the Lambda value,the  parameters that have more effect, shrink less, while that in case of less important parameters, the less important parameters shrink a lot near to 0, but not 0.\n",
    "\n",
    "#### In case of Lasso Regression we minimize :\n",
    "    The sum of squared residuals + (Lambda * (sum of |slope|))\n",
    "For the same case as above, the less important parameters will shrink to 0, and we are left with only the important parameters. The reason is, in this case we have to minimize the sum of |slope|, while in case of ridge regression we have to minimize the sum of |slope^2|.\n",
    " \n",
    "### From the above equation, it's clear that Lasso Regression can exclude useless variables from equations, so, it's little better than Ridge Regression at reducing the variance in models that, contain a lot of useless variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
